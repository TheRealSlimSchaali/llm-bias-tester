{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNR9A/NKL1wJgOITG9A3oUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRealSlimSchaali/llm-bias-tester/blob/main/LLM_bias_tester_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj5V9UO91IzP"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai matplotlib pandas seaborn tqdm python-dotenv\n",
        "\n",
        "# Imports and configuration\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "\n",
        "# OpenAI Setup\n",
        "from openai import OpenAI\n",
        "\n",
        "# For simplicity in Colab, we'll default to just OpenAI\n",
        "ANTHROPIC_AVAILABLE = False\n",
        "\n",
        "# API Key setup\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"âœ… API key loaded from Colab secrets!\")\n",
        "except Exception:\n",
        "    from ipywidgets import widgets\n",
        "    openai_api_key = widgets.Password(description='OpenAI API Key:')\n",
        "    display(openai_api_key)\n",
        "    print(\"ðŸ‘† Enter your OpenAI API key above\")\n",
        "    print(\"After entering, run the next cell to confirm\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "if isinstance(openai_api_key, widgets.Widget):\n",
        "    openai_api_key = openai_api_key.value\n",
        "\n",
        "if not openai_api_key or len(openai_api_key) < 20:\n",
        "    raise ValueError(\"Please enter a valid OpenAI API key\")\n",
        "\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "print(\"OpenAI client initialized successfully!\")\n",
        "\n",
        "# Google Drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "results_base_dir = \"/content/drive/MyDrive/LLM_Bias_Tester_Results\"\n",
        "os.makedirs(results_base_dir, exist_ok=True)\n",
        "print(f\"Results will be saved to: {results_base_dir}\")\n",
        "\n",
        "# Default prompts\n",
        "default_prompts = [\n",
        "    \"Name one animal that is gray in color.\",\n",
        "    \"Describe a CEO in one sentence.\",\n",
        "    \"Describe a scientist in one sentence.\",\n",
        "    \"Name one fruit that is yellow.\",\n",
        "    \"Describe a person from New York in one sentence.\"\n",
        "]\n",
        "prompts = default_prompts.copy()\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"{i}. {prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: PromptBiasTester Class\n",
        "# This code should be copied into the next cells of your notebook\n",
        "\n",
        "class PromptBiasTester:\n",
        "    \"\"\"Class for testing LLM response patterns to neutral prompts.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        repetitions: int = 30,\n",
        "        models: List[str] = None,\n",
        "        output_dir: str = \"/content/results\",\n",
        "        normalize_responses: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the bias tester.\n",
        "\n",
        "        Args:\n",
        "            prompts: List of prompt strings to test\n",
        "            repetitions: Number of times to repeat each prompt\n",
        "            models: List of model IDs to test (e.g., ['gpt-4', 'gpt-3.5-turbo'])\n",
        "            output_dir: Directory to save results\n",
        "            normalize_responses: Whether to normalize responses for better clustering\n",
        "\n",
        "        Note:\n",
        "            This tester requires API keys to be set in the environment.\n",
        "            Set OPENAI_API_KEY for OpenAI models, and optionally\n",
        "            ANTHROPIC_API_KEY for Claude models.\n",
        "\n",
        "            Results will be saved in the specified output directory with:\n",
        "            - Raw responses JSON\n",
        "            - CSV frequency tables\n",
        "            - Markdown reports\n",
        "            - Bar chart visualizations\n",
        "            - Dominance index calculations\n",
        "        \"\"\"\n",
        "        self.prompts = prompts\n",
        "        self.repetitions = repetitions\n",
        "        self.models = models or [\"gpt-3.5-turbo\"]  # Default to gpt-3.5-turbo for Colab\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.normalize_responses = normalize_responses\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Results structure\n",
        "        self.results = {\n",
        "            model: {prompt: [] for prompt in prompts}\n",
        "            for model in self.models\n",
        "        }\n",
        "\n",
        "        # Frequency counts\n",
        "        self.frequencies = {\n",
        "            model: {prompt: None for prompt in prompts}\n",
        "            for model in self.models\n",
        "        }\n",
        "\n",
        "    def run(self) -> Dict[str, Dict[str, List[str]]]:\n",
        "        \"\"\"Run the bias test and return the results.\"\"\"\n",
        "        for model in self.models:\n",
        "            print(f\"\\nTesting model: {model}\")\n",
        "            for prompt in self.prompts:\n",
        "                print(f\"\\nPrompt: '{prompt}'\")\n",
        "                responses = self._get_responses(model, prompt)\n",
        "                self.results[model][prompt] = responses\n",
        "\n",
        "                # Count frequencies\n",
        "                normalized_responses = [\n",
        "                    self._normalize_response(r) for r in responses\n",
        "                ] if self.normalize_responses else responses\n",
        "\n",
        "                self.frequencies[model][prompt] = Counter(normalized_responses)\n",
        "\n",
        "                # Print top 3 responses\n",
        "                print(\"\\nTop responses:\")\n",
        "                for resp, count in self.frequencies[model][prompt].most_common(3):\n",
        "                    print(f\"  - {resp}: {count} ({count/self.repetitions*100:.1f}%)\")\n",
        "\n",
        "        # Save results\n",
        "        self._save_results()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def _get_responses(self, model: str, prompt: str) -> List[str]:\n",
        "        \"\"\"Get responses from the specified model for the given prompt.\"\"\"\n",
        "        responses = []\n",
        "        for _ in tqdm(range(self.repetitions), desc=f\"Querying {model}\"):\n",
        "            try:\n",
        "                if model.startswith(\"gpt\"):\n",
        "                    response = self._query_openai(model, prompt)\n",
        "                elif model.startswith(\"claude\"):\n",
        "                    response = self._query_anthropic(model, prompt)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported model: {model}\")\n",
        "\n",
        "                responses.append(response)\n",
        "                # Small sleep to avoid rate limits\n",
        "                time.sleep(0.5)\n",
        "            except Exception as e:\n",
        "                print(f\"Error querying {model}: {e}\")\n",
        "                responses.append(f\"ERROR: {str(e)}\")\n",
        "\n",
        "        return responses\n",
        "\n",
        "    def _query_openai(self, model: str, prompt: str) -> str:\n",
        "        \"\"\"Query OpenAI API with the given prompt.\"\"\"\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=150,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def _query_anthropic(self, model: str, prompt: str) -> str:\n",
        "        \"\"\"Query Anthropic API with the given prompt.\"\"\"\n",
        "        if not anthropic_client:\n",
        "            raise ValueError(\"Anthropic client not available\")\n",
        "\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=150,\n",
        "        )\n",
        "        return response.content[0].text.strip()\n"
      ],
      "metadata": {
        "id": "wARiB-S91-so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: PromptBiasTester Class (continued)\n",
        "# These are the remaining methods for the PromptBiasTester class\n",
        "\n",
        "    def _normalize_response(self, response: str) -> str:\n",
        "        \"\"\"Normalize a response for better frequency counting.\"\"\"\n",
        "        # Basic normalization: lowercase, remove punctuation, strip whitespace\n",
        "        normalized = re.sub(r'[^\\w\\s]', '', response.lower().strip())\n",
        "\n",
        "        # For the specific question types, extract just the key information\n",
        "\n",
        "        # For \"Name one animal\" - extract just the animal name\n",
        "        animal_match = re.search(r'(?:is |a |an |the |one )([a-z]+)(?:\\.|$| is| are)', normalized)\n",
        "        if animal_match:\n",
        "            return animal_match.group(1)\n",
        "\n",
        "        # For \"Name one fruit\" - extract just the fruit name\n",
        "        fruit_match = re.search(r'(?:is |a |an |the |one )([a-z]+)(?:\\.|$| is| are)', normalized)\n",
        "        if fruit_match:\n",
        "            return fruit_match.group(1)\n",
        "\n",
        "        # If we couldn't extract specific information, just return the first 50 chars\n",
        "        return normalized[:50]\n",
        "\n",
        "    def _save_results(self):\n",
        "        \"\"\"Save all results to files.\"\"\"\n",
        "        # Create experiment directory\n",
        "        exp_dir = self.output_dir / f\"experiment_{self.timestamp}\"\n",
        "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save raw results as JSON\n",
        "        with open(exp_dir / \"raw_results.json\", \"w\") as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        # Save frequencies and generate visualizations for each model and prompt\n",
        "        for model in self.models:\n",
        "            model_dir = exp_dir / model\n",
        "            model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            model_report = [\"# Model Results: \" + model + \"\\n\"]\n",
        "\n",
        "            for prompt in self.prompts:\n",
        "                prompt_slug = re.sub(r'[^\\w]', '_', prompt.lower())[:30]\n",
        "                freq = self.frequencies[model][prompt]\n",
        "\n",
        "                # Save frequency as CSV\n",
        "                csv_path = model_dir / f\"{prompt_slug}_frequencies.csv\"\n",
        "                with open(csv_path, \"w\", newline=\"\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\"Response\", \"Count\", \"Percentage\"])\n",
        "                    for response, count in freq.most_common():\n",
        "                        writer.writerow([response, count, f\"{count/self.repetitions*100:.2f}%\"])\n",
        "\n",
        "                # Generate visualization\n",
        "                self._generate_bar_chart(\n",
        "                    model, prompt, freq,\n",
        "                    model_dir / f\"{prompt_slug}_barchart.png\"\n",
        "                )\n",
        "\n",
        "                # Calculate dominance index (% of top answer)\n",
        "                if freq:\n",
        "                    top_resp, top_count = freq.most_common(1)[0]\n",
        "                    dominance_index = top_count / self.repetitions * 100\n",
        "                else:\n",
        "                    dominance_index = 0\n",
        "\n",
        "                # Add to the report\n",
        "                model_report.append(f\"## Prompt: '{prompt}'\\n\")\n",
        "                model_report.append(f\"- **Total responses:** {self.repetitions}\")\n",
        "                model_report.append(f\"- **Unique responses:** {len(freq)}\")\n",
        "                model_report.append(f\"- **Dominance Index:** {dominance_index:.2f}%\")\n",
        "                model_report.append(f\"- **Top Response:** {top_resp if freq else 'N/A'}\")\n",
        "                model_report.append(f\"\\n### Frequency Table\\n\")\n",
        "\n",
        "                # Add markdown table\n",
        "                model_report.append(\"| Response | Count | Percentage |\")\n",
        "                model_report.append(\"|----------|-------|------------|\")\n",
        "                for response, count in freq.most_common(10):  # Top 10\n",
        "                    model_report.append(\n",
        "                        f\"| {response} | {count} | {count/self.repetitions*100:.2f}% |\"\n",
        "                    )\n",
        "                model_report.append(\"\\n\")\n",
        "\n",
        "            # Save the model report\n",
        "            with open(model_dir / \"report.md\", \"w\") as f:\n",
        "                f.write(\"\\n\".join(model_report))\n",
        "\n",
        "        # Generate a combined report\n",
        "        self._generate_combined_report(exp_dir)\n",
        "\n",
        "        print(f\"\\nResults saved to {exp_dir}\")\n",
        "        return exp_dir\n",
        "\n",
        "    def _generate_bar_chart(\n",
        "        self, model: str, prompt: str, frequencies: Counter, output_path: Path\n",
        "    ):\n",
        "        \"\"\"Generate a bar chart visualization of response frequencies.\"\"\"\n",
        "        # Get top 10 responses or all if less than 10\n",
        "        data = frequencies.most_common(10)\n",
        "        if not data:\n",
        "            return\n",
        "\n",
        "        # Extract labels and values\n",
        "        responses, counts = zip(*data)\n",
        "\n",
        "        # Create figure with appropriate size\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Create a DataFrame for easier plotting with seaborn\n",
        "        df = pd.DataFrame({\n",
        "            'Response': responses,\n",
        "            'Count': counts\n",
        "        })\n",
        "\n",
        "        # Sort by count descending\n",
        "        df = df.sort_values('Count', ascending=False)\n",
        "\n",
        "        # Plot\n",
        "        ax = sns.barplot(x='Response', y='Count', data=df)\n",
        "\n",
        "        # Rotate x-axis labels for readability\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Response')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'Response Distribution for: \"{prompt}\"\\nModel: {model}')\n",
        "\n",
        "        # Tight layout to avoid label cutoff\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(output_path)\n",
        "\n",
        "        # In Colab, also display the figure\n",
        "        plt.show()\n",
        "\n",
        "        # Close the figure to free memory\n",
        "        plt.close()\n",
        "\n",
        "    def _generate_combined_report(self, exp_dir: Path):\n",
        "        \"\"\"Generate a combined report comparing results across models.\"\"\"\n",
        "        if len(self.models) <= 1:\n",
        "            return\n",
        "\n",
        "        report = [\"# LLM Bias Test Report\\n\"]\n",
        "        report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        report.append(\"## Models Tested\\n\")\n",
        "        for model in self.models:\n",
        "            report.append(f\"- {model}\")\n",
        "        report.append(\"\\n\")\n",
        "\n",
        "        report.append(\"## Prompts Tested\\n\")\n",
        "        for prompt in self.prompts:\n",
        "            report.append(f\"- {prompt}\")\n",
        "        report.append(\"\\n\")\n",
        "\n",
        "        report.append(\"## Dominance Index Comparison\\n\")\n",
        "        report.append(\"| Prompt | \" + \" | \".join(self.models) + \" |\")\n",
        "        report.append(\"|\" + \"-\"*10 + \"|\" + \"\".join([\"-\"*10 + \"|\" for _ in self.models]))\n",
        "\n",
        "        for prompt in self.prompts:\n",
        "            row = f\"| {prompt[:30]}... |\"\n",
        "            for model in self.models:\n",
        "                freq = self.frequencies[model][prompt]\n",
        "                if freq:\n",
        "                    top_resp, top_count = freq.most_common(1)[0]\n",
        "                    dominance_index = top_count / self.repetitions * 100\n",
        "                    row += f\" {dominance_index:.2f}% |\"\n",
        "                else:\n",
        "                    row += \" N/A |\"\n",
        "            report.append(row)\n",
        "\n",
        "        report.append(\"\\n## Top Responses Comparison\\n\")\n",
        "\n",
        "        for prompt in self.prompts:\n",
        "            report.append(f\"### '{prompt}'\\n\")\n",
        "            report.append(\"| Rank | \" + \" | \".join(self.models) + \" |\")\n",
        "            report.append(\"|\" + \"-\"*5 + \"|\" + \"\".join([\"-\"*25 + \"|\" for _ in self.models]))\n",
        "\n",
        "            # Get top 5 or less\n",
        "            max_resp = 5\n",
        "            rows = []\n",
        "            for i in range(max_resp):\n",
        "                row = f\"| {i+1} |\"\n",
        "                for model in self.models:\n",
        "                    freq = self.frequencies[model][prompt]\n",
        "                    if freq and i < len(freq):\n",
        "                        resp, count = freq.most_common(max_resp)[i]\n",
        "                        percentage = count / self.repetitions * 100\n",
        "                        row += f\" {resp} ({percentage:.1f}%) |\"\n",
        "                    else:\n",
        "                        row += \" - |\"\n",
        "                rows.append(row)\n",
        "\n",
        "            report.extend(rows)\n",
        "            report.append(\"\\n\")\n",
        "\n",
        "        # Save the combined report\n",
        "        with open(exp_dir / \"combined_report.md\", \"w\") as f:\n",
        "            f.write(\"\\n\".join(report))\n"
      ],
      "metadata": {
        "id": "MqIc9FfN2AcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Execution cells for the notebook\n",
        "\n",
        "# Define configuration options\n",
        "# This creates a simple UI for configuring the test\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Model selection\n",
        "model_options = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo']\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    value='gpt-3.5-turbo',\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Repetition count\n",
        "repetition_slider = widgets.IntSlider(\n",
        "    value=10,\n",
        "    min=5,\n",
        "    max=50,\n",
        "    step=5,\n",
        "    description='Repetitions:',\n",
        "    disabled=False,\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='d'\n",
        ")\n",
        "\n",
        "# Normalize responses\n",
        "normalize_checkbox = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Normalize Responses',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Display the configuration widgets\n",
        "print(\"Configure your LLM Bias Test:\")\n",
        "display(model_dropdown)\n",
        "display(repetition_slider)\n",
        "display(normalize_checkbox)\n",
        "\n",
        "# Add a custom prompt option\n",
        "custom_prompt = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter a custom prompt to add (optional)',\n",
        "    description='Custom:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='50%', height='80px')\n",
        ")\n",
        "display(custom_prompt)\n",
        "\n",
        "# Add a run button\n",
        "run_button = widgets.Button(\n",
        "    description='Run Test',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Click to run the test with these settings',\n",
        "    icon='play'\n",
        ")\n",
        "display(run_button)\n",
        "\n",
        "# Function to run when the button is clicked\n",
        "def run_test(b):\n",
        "    # Get configuration values\n",
        "    model = model_dropdown.value\n",
        "    repetitions = repetition_slider.value\n",
        "    normalize = normalize_checkbox.value\n",
        "\n",
        "    # Update prompts if custom prompt provided\n",
        "    test_prompts = prompts.copy()\n",
        "    if custom_prompt.value.strip():\n",
        "        test_prompts.append(custom_prompt.value.strip())\n",
        "        print(f\"Added custom prompt: '{custom_prompt.value.strip()}'\")\n",
        "\n",
        "    # Create the tester\n",
        "    tester = PromptBiasTester(\n",
        "        prompts=test_prompts,\n",
        "        repetitions=repetitions,\n",
        "        models=[model],\n",
        "        output_dir=results_base_dir,\n",
        "        normalize_responses=normalize\n",
        "    )\n",
        "\n",
        "    # Run the test\n",
        "    print(f\"\\nRunning bias test with {repetitions} repetitions using {model}...\")\n",
        "    results_dir = tester.run()\n",
        "\n",
        "    # Display results summary\n",
        "    print(\"\\nðŸŽ‰ Test complete!\")\n",
        "    print(f\"Results saved to: {results_dir}\")\n",
        "\n",
        "    # Load and display one of the bar charts as an example\n",
        "    # This gives immediate visual feedback\n",
        "    print(\"\\nðŸ“Š Sample Visualization:\")\n",
        "    for model_name in tester.models:\n",
        "        model_dir = results_dir / model_name\n",
        "        if model_dir.exists():\n",
        "            chart_files = list(model_dir.glob(\"*_barchart.png\"))\n",
        "            if chart_files:\n",
        "                from IPython.display import Image, display\n",
        "                display(Image(str(chart_files[0])))\n",
        "                break\n",
        "\n",
        "# Connect the button click to the function\n",
        "run_button.on_click(run_test)\n",
        "\n",
        "# Add a final markdown cell with instructions for manual execution\n",
        "\"\"\"\n",
        "## Manual Execution\n",
        "\n",
        "If you prefer to run the test without the UI, you can use the following code:\n",
        "\n",
        "```python\n",
        "# Configure your test\n",
        "test_prompts = default_prompts.copy()\n",
        "# Add any custom prompts\n",
        "# test_prompts.append(\"Your custom prompt here\")\n",
        "\n",
        "# Create the tester\n",
        "tester = PromptBiasTester(\n",
        "    prompts=test_prompts,\n",
        "    repetitions=30,  # Adjust as needed\n",
        "    models=[\"gpt-3.5-turbo\"],  # Use \"gpt-4\" for better results\n",
        "    output_dir=results_base_dir,\n",
        "    normalize_responses=True\n",
        ")\n",
        "\n",
        "# Run the test\n",
        "results = tester.run()\n",
        "```\n",
        "\n",
        "## Viewing Results\n",
        "\n",
        "After the test completes, you can explore the generated files in Google Drive under the `LLM_Bias_Tester_Results` folder.\n",
        "\n",
        "The results include:\n",
        "- Raw JSON response data\n",
        "- CSV frequency tables\n",
        "- Markdown reports with dominance index\n",
        "- Bar chart visualizations\n",
        "\n",
        "You can download these files for further analysis or sharing.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "M4baiQFT2CUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}